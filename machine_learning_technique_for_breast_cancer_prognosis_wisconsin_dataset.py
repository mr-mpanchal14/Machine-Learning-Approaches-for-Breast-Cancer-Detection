# -*- coding: utf-8 -*-
"""Machine Learning Technique for Breast Cancer Prognosis - Wisconsin Dataset

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nUYUt8tqAjjrI3u5x-7Uu7FIIJZlVagu

# Importing Dataset
"""

# for mathematical calculations
import numpy as np

# to import and manipulate the dataset
import pandas as pd

# to visualize the data and results
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import seaborn as sns

# to build the model
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc
from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis, LocalOutlierFactor
from sklearn.decomposition import PCA

# to get rid of the warnings
import warnings
warnings.filterwarnings("ignore")

data = pd.read_csv('/content/drive/MyDrive/Breast Cancer Prognosis/wisconsin/data.csv')

print(data["diagnosis"].value_counts())
print("Total:", len(data["diagnosis"]))

sns.countplot(x = data["diagnosis"])
plt.xlabel('Diagnosis')
plt.ylabel('Count')
plt.title('Distribution of the Target variable between Benign(B) & Malignant(M)')
plt.show()

# Import the dataset
data = pd.read_csv('/content/drive/MyDrive/Breast Cancer Prognosis/wisconsin/data.csv')

data.drop(["Unnamed: 32", "id"], inplace = True, axis = 1)

data.rename(columns = {"diagnosis": "target"}, inplace = True)

data.head()

data.info()

# List comprehension method to convert string data of "Diagnosis" column to int
data["target"] = [1 if i.strip() == 'M' else 0 for i in data["target"]]

data.head()

"""# EDA"""

print(data["target"].value_counts())
print("Total:", len(data["target"]))

sns.countplot(x = data["target"])

plt.show()

print("Dataset Shape:", data.shape)

data.info()

data.describe()

data.isna().sum()

# simplified correlation matrix

threshold = 0.65
corrMatrix = data.corr()
filt = np.abs(corrMatrix["target"]) > threshold
corrFeatures = corrMatrix.columns[filt].tolist()

sns.clustermap(data[corrFeatures].corr(), annot = True, fmt = ".3f")
plt.title("Correlation Between the Features with Threshold 0.65")

plt.show()

sns.pairplot(data[corrFeatures], diag_kind = "kde", markers = '+', hue = "target")

plt.show()

# outlier detection

y = data["target"]
x = data.drop(["target"], axis = 1)

clf = LocalOutlierFactor(n_neighbors = 20)
clf.fit_predict(x)
xScore = clf.negative_outlier_factor_

outlierScore = pd.DataFrame()
outlierScore["score"] = xScore

threshold = -2.5
filt = outlierScore["score"] < threshold
outlierIndex = outlierScore[filt].index.tolist()

radius = (xScore.max() - xScore) / (xScore.max() - xScore.min())
outlierScore["radius"] = radius

plt.scatter(x.iloc[:, 0], x.iloc[:, 1], color = 'k', s = 3, label = "Data Points")
plt.scatter(x.iloc[:, 0], x.iloc[:, 1], s = 1000 * radius, edgecolors = 'r', facecolors = "none", label = "Oulier Scores")
plt.scatter(x.iloc[outlierIndex, 0], x.iloc[outlierIndex, 1], color = 'b', s = 50, label = "Outliers")
plt.legend()

plt.show()

"""As you see above, we detected the outliers by using the method of LOF. According to the LOF, the data whose negative outlier factor is less than -1 is an outlier. However, we used the threshold value of -2.5 instead of -1 in order to prevent the loss of data. Now, let's drop the outliers from the dataset."""

# drop outliers

x = x.drop(outlierIndex)
y = y.drop(outlierIndex).values

"""# Train-Test-Split"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = 42)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

# fit gets the mean and standard deviation, transform applies the standardization formula to each values
x_train = sc.fit_transform(x_train)

# we will apply this same standard scaler to the x_test and just use the transform function
x_test = sc.transform(x_test)

"""# Correlation"""

# Build a Dataframe with Correlation between Features
corr_matrix = x.corr()
# Take absolute values of correlated coefficients
corr_matrix = corr_matrix.abs().unstack()
corr_matrix = corr_matrix.sort_values(ascending=False)
# Take only features with correlation above threshold of 0.8
corr_matrix = corr_matrix[corr_matrix >= 0.8]
corr_matrix = corr_matrix[corr_matrix < 1]
corr_matrix = pd.DataFrame(corr_matrix).reset_index()
corr_matrix.columns = ['feature1', 'feature2', 'Correlation']
corr_matrix.head()

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)

# Get groups of features that are correlated amongs themselves
grouped_features = []
correlated_groups = []

for feature in corr_matrix.feature1.unique():
    if feature not in grouped_features:
        # Find all features correlated to a single feature
        correlated_block = corr_matrix[corr_matrix.feature1 == feature]
        grouped_features = grouped_features + list(correlated_block.feature2.unique()) + [feature]
        
        # Append block of features to the list
        correlated_groups.append(correlated_block)

print('Found {} correlated feature groups'.format(len(correlated_groups)))
print('out of {} total features.'.format(x_train.shape[1]))

# Visualize Correlated Feature Groups
for group in correlated_groups:
    print(group)
    print('\n')

data.drop("perimeter_mean",axis=1,inplace=True)

## Normalising skewed rows
data.skew(axis = 0)

data['radius_se']=np.log(data['radius_se'])
data['perimeter_se']=np.log(data['perimeter_se'])
data['area_se']=np.log(data['area_se'])
data['concavity_se']=np.log(data['concavity_se'])

data.drop(["concavity_se"], inplace = True, axis = 1)

data.skew(axis=0)

X=df.iloc[:,2:23]

X

y=df['Recurrence']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)



"""# Building ANN"""

import tensorflow as tf

ann = tf.keras.models.Sequential()

ann.add(tf.keras.layers.Dense(units=6, activation='relu'))

ann.add(tf.keras.layers.Dense(units=6, activation='relu'))

ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

ann.fit(X_train, y_train, batch_size = 32, epochs = 100)

y_pred = ann.predict(X_test)
y_pred = (y_pred > 0.5)

"""# MODELS"""

# Logistic Regression - 97.34
from sklearn.linear_model import LogisticRegression
log_model = LogisticRegression(random_state=42)
log_model.fit(x_train, y_train)

#KNN - 95.74
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=5, metric = 'minkowski', p = 2)
classifier.fit(x_train, y_train)

#SVM - 97.87
from sklearn.svm import SVC
classifier = SVC(kernel='linear', random_state = 0)
classifier.fit(x_train, y_train)

#KernelSVM - 96.8
from sklearn.svm import SVC
classifier = SVC(kernel='rbf', random_state = 0)
classifier.fit(x_train, y_train)

# Decision Tree - 92.8
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier()
classifier.fit(x_train, y_train)

# RandomForest - 97.3
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 100, random_state = 0)
classifier.fit(x_train, y_train)

y_pred = classifier.predict(x_test)

# XGboost - 96.8
from xgboost import XGBClassifier
classifier = XGBClassifier()
classifier.fit(x_train, y_train)

from keras.models import Sequential
from keras.layers import Dense

model = Sequential() 
model.add(Dense(128, activation='relu', input_dim=30))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) 
model.summary()

hist = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=60, batch_size=100)

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

sns.set()
acc = hist.history['accuracy']
val = hist.history['val_accuracy']
epochs = range(1, len(acc) + 1)

plt.figure(figsize=(12, 8))
plt.plot(epochs, acc, '-', label='Training accuracy')
plt.plot(epochs, val, ':', label='Validation accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.plot()

"""# Fine Tune models"""

# LR - fine-tune the model - The best training score is 0.977 with the parameters {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}.
# Test Score: 0.984, Train Score: 0.989

from sklearn.model_selection import RepeatedStratifiedKFold
def lr_best_params(x_train, x_test, y_train, y_test):
    # create the params that we will change

    # define models and parameters
    solvers = ['newton-cg', 'lbfgs', 'liblinear']
    penalty = ['l2']
    c_values = [100, 10, 1.0, 0.1, 0.01]
    
    paramGrid = dict(solver=solvers,
                     penalty=penalty,
                     C=c_values)
    
    # create a new LR model
    lr = LogisticRegression()
    
    # thanks to Grid Search, we can try our params.
    # define grid search
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
    grid = GridSearchCV(estimator=lr, param_grid=paramGrid, n_jobs=-1, cv=cv, scoring='accuracy', error_score=0)
    grid.fit(x_train, y_train)
    
    print("The best training score is {:.3f} with the parameters {}.".format(grid.best_score_, 
                                                                          grid.best_params_))
    
    # create another KNN model, and fit the model with 
    # the params that we found above.
    lr = LogisticRegression(**grid.best_params_)
    lr.fit(x_train, y_train)
    
    yPredTest = lr.predict(x_test)
    yPredTrain = lr.predict(x_train)
    
    # create confusion matrices
    cmTest = confusion_matrix(y_test, yPredTest)
    cmTrain = confusion_matrix(y_train, yPredTrain)
    
    # calculate the accuricies
    accTest = accuracy_score(y_test, yPredTest)
    accTrain = accuracy_score(y_train, yPredTrain)
    print("Test Score: {:.3f}, Train Score: {:.3f}".format(accTest, accTrain))
    print(classification_report(y_test, yPredTest))
    #print(f1_score(y_test, yPredTest, average="macro"))
    #print(precision_score(y_test, yPredTest, average="macro"))
    #print(recall_score(y_test, yPredTest, average="macro"))
    
    # visualize confusion matrices 
    fig, ax = plt.subplots(1, 2, figsize=(16, 8))
    
    ax[0].matshow(cmTest, cmap = plt.cm.Blues, alpha = 0.3)
    for i in range(cmTest.shape[0]):
        for j in range(cmTest.shape[1]):
            ax[0].text(x = j, y = i, s = cmTest[i, j], va = 'center', ha = 'center', size = 'large', fontsize = 25)
 
    ax[0].set_title('Test Confusion Matrix', fontsize = 18)
    ax[0].set_xlabel('Predictions', fontsize = 20)
    ax[0].set_ylabel('Actuals', fontsize = 20)
    
    ax[1].matshow(cmTrain, cmap = plt.cm.Blues, alpha = 0.3)
    for i in range(cmTrain.shape[0]):
        for j in range(cmTrain.shape[1]):
            ax[1].text(x = j, y = i, s = cmTrain[i, j], va = 'center', ha = 'center', size = 'large', fontsize = 25)
 
    ax[1].set_title('Train Confusion Matrix', fontsize = 18)
    ax[1].set_xlabel('Predictions', fontsize = 20)
    ax[1].set_ylabel('Actuals', fontsize = 20)

    plt.show()

    # calculate the fpr and tpr for all thresholds of the classification

    fpr, tpr, threshold = roc_curve(y_test, yPredTest)
    roc_auc = auc(fpr, tpr)

    # method I: plt 
    plt.title('LR - Receiver Operating Characteristic')
    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
    plt.legend(loc = 'lower right')
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')
    
    #plt.tight_layout()
    plt.show()
    print("\nAUC: ", roc_auc)
    
    return grid

grid = lr_best_params(x_train, x_test, y_train, y_test)

# KNN - fine-tune the model - 99.4

def knn_best_params(x_train, x_test, y_train, y_test):
    # create the params that we will change
    kRange = list(range(1, 31))
    weightOptions = ["uniform", "distance"]
    algorithmOptions = ["auto", "ball_tree", "kd_tree", "brute"]
    pOptions = [1, 2]
    
    paramGrid = dict(n_neighbors = kRange, 
                     weights = weightOptions,
                     algorithm = algorithmOptions,
                     p = pOptions)
    
    # create a new KNNN model
    knn = KNeighborsClassifier()
    
    # thanks to Grid Search, we can try our params.
    grid = GridSearchCV(knn, paramGrid, cv = 10, scoring = "accuracy")
    grid.fit(x_train, y_train)
    
    print("The best training score is {:.3f} with the parameters {}.".format(grid.best_score_, 
                                                                          grid.best_params_))
    
    # create another KNN model, and fit the model with 
    # the params that we found above.
    knn = KNeighborsClassifier(**grid.best_params_)
    knn.fit(x_train, y_train)
    
    yPredTest = knn.predict(x_test)
    yPredTrain = knn.predict(x_train)
    
    # create confusion matrices
    cmTest = confusion_matrix(y_test, yPredTest)
    cmTrain = confusion_matrix(y_train, yPredTrain)
    
    # calculate the accuricies
    accTest = accuracy_score(y_test, yPredTest)
    accTrain = accuracy_score(y_train, yPredTrain)
    print("Test Score: {:.3f}, Train Score: {:.3f}".format(accTest, accTrain))
    print(classification_report(y_test, yPredTest))
    
    # visualize confusion matrices 
    fig, ax = plt.subplots(1, 2, figsize=(16, 8))
    
    ax[0].matshow(cmTest, cmap = plt.cm.Blues, alpha = 0.3)
    for i in range(cmTest.shape[0]):
        for j in range(cmTest.shape[1]):
            ax[0].text(x = j, y = i, s = cmTest[i, j], va = 'center', ha = 'center', size = 'large', fontsize = 25)
 
    ax[0].set_title('Test Confusion Matrix', fontsize = 18)
    ax[0].set_xlabel('Predictions', fontsize = 20)
    ax[0].set_ylabel('Actuals', fontsize = 20)
    
    ax[1].matshow(cmTrain, cmap = plt.cm.Blues, alpha = 0.3)
    for i in range(cmTrain.shape[0]):
        for j in range(cmTrain.shape[1]):
            ax[1].text(x = j, y = i, s = cmTrain[i, j], va = 'center', ha = 'center', size = 'large', fontsize = 25)
 
    ax[1].set_title('Train Confusion Matrix', fontsize = 18)
    ax[1].set_xlabel('Predictions', fontsize = 20)
    ax[1].set_ylabel('Actuals', fontsize = 20)

    plt.show()

    # calculate the fpr and tpr for all thresholds of the classification

    fpr, tpr, threshold = roc_curve(y_test, yPredTest)
    roc_auc = auc(fpr, tpr)

    # method I: plt 
    plt.title('KNN - Receiver Operating Characteristic')
    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
    plt.legend(loc = 'lower right')
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')

    
    #plt.tight_layout()
    plt.show()
    print("\nAUC: ", roc_auc)
    
    return grid

grid = knn_best_params(x_train, x_test, y_train, y_test)

# RandomForest - fine-tune the model - 98.8

def rf_best_params(x_train, x_test, y_train, y_test):
    # create the params that we will change
    nRange = list(range(50,1000,50))
    ct = ["gini", "entropy", "log_loss"]
    mfeatures = ["sqrt", "log2"]
    
    paramGrid = dict(n_estimators = [100,500,650,1000], 
                     criterion = ct,
                     max_features = mfeatures)
    
    # create a new KNNN model
    from sklearn.ensemble import RandomForestClassifier
    rf = RandomForestClassifier(random_state = 42)
    
    # thanks to Grid Search, we can try our params.
    grid = GridSearchCV(rf, paramGrid, cv = 10, scoring = "accuracy")
    grid.fit(x_train, y_train)
    
    print("The best training score is {:.3f} with the parameters {}.".format(grid.best_score_, 
                                                                          grid.best_params_))
    
    # create another KNN model, and fit the model with 
    # the params that we found above.
    rf = RandomForestClassifier(**grid.best_params_)
    rf.fit(x_train, y_train)
    
    yPredTest = rf.predict(x_test)
    yPredTrain = rf.predict(x_train)
    
    # create confusion matrices
    cmTest = confusion_matrix(y_test, yPredTest)
    cmTrain = confusion_matrix(y_train, yPredTrain)
    
    # calculate the accuricies
    accTest = accuracy_score(y_test, yPredTest)
    accTrain = accuracy_score(y_train, yPredTrain)
    print("Test Score: {:.3f}, Train Score: {:.3f}".format(accTest, accTrain))
    print(classification_report(y_test, yPredTest))
    
    # visualize confusion matrices 
    fig, ax = plt.subplots(1, 2, figsize=(16, 8))
    
    ax[0].matshow(cmTest, cmap = plt.cm.Blues, alpha = 0.3)
    for i in range(cmTest.shape[0]):
        for j in range(cmTest.shape[1]):
            ax[0].text(x = j, y = i, s = cmTest[i, j], va = 'center', ha = 'center', size = 'large', fontsize = 25)
 
    ax[0].set_title('Test Confusion Matrix', fontsize = 18)
    ax[0].set_xlabel('Predictions', fontsize = 20)
    ax[0].set_ylabel('Actuals', fontsize = 20)
    
    ax[1].matshow(cmTrain, cmap = plt.cm.Blues, alpha = 0.3)
    for i in range(cmTrain.shape[0]):
        for j in range(cmTrain.shape[1]):
            ax[1].text(x = j, y = i, s = cmTrain[i, j], va = 'center', ha = 'center', size = 'large', fontsize = 25)
 
    ax[1].set_title('Train Confusion Matrix', fontsize = 18)
    ax[1].set_xlabel('Predictions', fontsize = 20)
    ax[1].set_ylabel('Actuals', fontsize = 20)

    plt.show()

    # calculate the fpr and tpr for all thresholds of the classification

    fpr, tpr, threshold = roc_curve(y_test, yPredTest)
    roc_auc = auc(fpr, tpr)

    # method I: plt 
    plt.title('RF - Receiver Operating Characteristic')
    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
    plt.legend(loc = 'lower right')
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')
    
    #plt.tight_layout()
    plt.show()
    print("\nAUC: ", roc_auc)
    
    return grid

grid = rf_best_params(x_train, x_test, y_train, y_test)

"""# PCA"""

# standardization

scaler = StandardScaler()
xScaled = scaler.fit_transform(x)

# dimension reduction with PCA

pca = PCA(n_components = 2)
xReducedPca = pca.fit_transform(xScaled)

pcaData = pd.DataFrame(xReducedPca, columns = ["p1", "p2"])
pcaData["target"] = y

sns.scatterplot(x = "p1", y = "p2", hue = "target", data = pcaData)
plt.title("PCA: p1 vs p2")

plt.show()

# create train test split for the PCA and run the KNN model 
# with the data we get from the PCA

xTrainPca, xTestPca, yTrainPca, yTestPca = train_test_split(xReducedPca, y, test_size = 0.3, random_state = 42)

gridPca = knn_best_params(xTrainPca, xTestPca, yTrainPca, yTestPca)

"""# NCA"""

# apply the NCA

nca = NeighborhoodComponentsAnalysis(n_components = 2, random_state = 42)
xReducedNca = nca.fit_transform(xScaled, y)

ncaData = pd.DataFrame(xReducedNca, columns = ["p1", "p2"])
ncaData["target"] = y

sns.scatterplot(x = "p1", y = "p2", hue = "target", data = ncaData)
plt.title("NCA: p1 vs p2")

plt.show()

# create train test split for the NCA and run the KNN model 
# with the data we get from the NCA

xTrainNca, xTestNca, yTrainNca, yTestNca = train_test_split(xReducedNca, y, test_size = 0.3, random_state = 42)

gridNca = lr_best_params(xTrainNca, xTestNca, yTrainPca, yTestNca)

"""# Prediction"""

y_pred = classifier.predict(x_test)

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
ac = accuracy_score(y_test, y_pred)
print(cm, '\n',ac)

